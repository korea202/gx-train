{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f70981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model          | ResNet             | 21.3 M | train\n",
      "1 | train_accuracy | MulticlassAccuracy | 0      | train\n",
      "2 | train_f1       | MulticlassF1Score  | 0      | train\n",
      "3 | val_accuracy   | MulticlassAccuracy | 0      | train\n",
      "4 | val_f1         | MulticlassF1Score  | 0      | train\n",
      "5 | criterion      | CrossEntropyLoss   | 0      | train\n",
      "--------------------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 M    Total params\n",
      "85.174    Total estimated model params size (MB)\n",
      "171       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a36f0a01854409a4e5506b9898ffa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/python_work/git/gx-train/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f201a2174ea749b6a9c17c46369a9a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aed56fad8d5478da270dcb75cdf2208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.217\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/data/ephemeral/home/python_work/git/gx-train/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902753ab97f544e69c78ab44e445bc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ê°¯ìˆ˜= 3140\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# í•˜ì´ë“œë¼ì™€ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì€ ì•„ê·œë¨¼íŠ¸ ê´€ë ¨ ì¶©ëŒì´ ë°œìƒí•˜ë¯€ë¡œ ì´ˆê¸°í™” í•´ì¤Œ\n",
    "sys.argv = ['']\n",
    "# í™˜ê²½ë³€ìˆ˜ ì½ê¸°\n",
    "if (python_path := dotenv_values().get('PYTHONPATH')) and python_path not in sys.path: sys.path.append(python_path)\n",
    "\n",
    "from src.dataset import ImageDataset\n",
    "from src.models.CustomModel import CustomModel\n",
    "from src.utils import config\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "def random_seed(seed_num=42):\n",
    "\n",
    "    \"\"\" SEED = seed_num\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True \"\"\"\n",
    "    \n",
    "    # seed_everything ì€ ìœ„ì˜ ë‚´ìš© ì œì–´ + ë°‘ì—ë‚´ìš©\n",
    "    pl.seed_everything(seed_num)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜\n",
    "def prepare_data(batch_size=32, num_workers=4):\n",
    "    \n",
    "   # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_dataset, val_dataset, test_dataset = ImageDataset.get_datasets()\n",
    "\n",
    "    # DataLoader ì •ì˜\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,  # ë³„ë„ì˜ ê²€ì¦ ë°ì´í„°ì…‹\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # ê²€ì¦ ì‹œì—ëŠ” ì…”í”Œí•˜ì§€ ì•ŠìŒ\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # model config\n",
    "    model_name = 'resnet34' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "    # training config\n",
    "    EPOCHS = 1\n",
    "    BATCH_SIZE = 32\n",
    "    num_workers = 0\n",
    "    num_classes = 17\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    random_seed(42)\n",
    "\n",
    "    # ë°ì´í„° ë¡œë” ì¤€ë¹„\n",
    "    train_loader, val_loader, test_loader = prepare_data(batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "    model = CustomModel(\n",
    "        model_name= model_name,\n",
    "        num_classes=num_classes,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "     # ì½œë°±ì„ ì§ì ‘ ìƒì„±\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            mode='min',\n",
    "            min_delta=0.001,\n",
    "            verbose=True\n",
    "        ),\n",
    "        LearningRateMonitor(\n",
    "            logging_interval='epoch',\n",
    "            log_momentum=False\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    trainer = Trainer(default_root_dir=config.OUTPUTS_DIR, max_epochs=EPOCHS, accelerator='auto', callbacks=callbacks)\n",
    "    \n",
    "    # í›ˆë ¨\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸\n",
    "    trainer.test(model, test_loader)\n",
    "\n",
    "    print(\"í…ŒìŠ¤íŠ¸ ê°¯ìˆ˜=\",len(model.test_predictions))\n",
    "    if len(model.test_predictions) > 0:\n",
    "        # ëª¨ë“  ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ í•©ì¹˜ê¸°\n",
    "        all_preds = model.test_predictions\n",
    "        \n",
    "        pred_df = pd.DataFrame(test_loader.dataset.df, columns=['ID', 'target'])\n",
    "        pred_df['target'] = all_preds\n",
    "\n",
    "        sample_submission_df = pd.read_csv(config.CV_CLS_TEST_CSV)\n",
    "        assert (sample_submission_df['ID'] == pred_df['ID']).all()\n",
    "        pred_df.to_csv(config.OUTPUTS_DIR + \"/pred.csv\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gx-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
