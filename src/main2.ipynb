{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3480a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # 기본적인 연산을 위한 라이브러리\n",
    "import matplotlib.pyplot as plt # 시각화를 위한 라이브러리\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm # 상태 바를 나타내기 위한 라이브러리\n",
    "\n",
    "import torch # PyTorch 라이브러리\n",
    "import torch.nn as nn # 모델 구성을 위한 라이브러리\n",
    "import torch.optim as optim # optimizer 설정을 위한 라이브러리\n",
    "from torch.utils.data import Dataset, DataLoader # 데이터셋 설정을 위한 라이브러리\n",
    "\n",
    "import torchvision # PyTorch의 컴퓨터 비전 라이브러리\n",
    "import torchvision.transforms as T # 이미지 변환을 위한 모듈\n",
    "\n",
    "from sklearn.metrics import accuracy_score # 성능지표 측정\n",
    "from sklearn.model_selection import train_test_split # train-validation-test set 나누는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1322472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "\n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 불러올 때, 필요한 변환(transform)을 정의합니다.\n",
    "mnist_transform = T.Compose([\n",
    "    T.ToTensor(), # 텐서 형식으로 변환\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv('../data/medium_data.csv')\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_csv.shape)\n",
    "\n",
    "# 각각의 title만 추출합니다.\n",
    "# 우리는 title의 첫 단어가 주어졌을 때, 다음 단어를 예측하는 것을 수행할 것입니다.\n",
    "data = data_csv['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f68081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleaning_text(text):\n",
    "    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n",
    "    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n",
    "    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_data = list(map(cleaning_text, data)) # 모든 특수문자와 공백을 지움\n",
    "print('Before preprocessing')\n",
    "print(data[:5])\n",
    "print('After preprocessing')\n",
    "print(cleaned_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52315dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # AutoTokenizer로 한 번에 처리\n",
    "        X_data, labels = self.create_sequences_with_padding(data)\n",
    "        self.X = torch.tensor(X_data)\n",
    "        self.label = torch.tensor(labels)\n",
    "\n",
    "    def create_sequences_with_padding(self, texts):\n",
    "        all_X = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # 토큰화 (특수 토큰 제외)\n",
    "            tokens = self.tokenizer(text, add_special_tokens=False)['input_ids']\n",
    "            \n",
    "            # 점진적 시퀀스 생성\n",
    "            for i in range(1, len(tokens)):\n",
    "                sequence = tokens[:i+1]\n",
    "                \n",
    "                # AutoTokenizer로 즉시 패딩 처리\n",
    "                if len(sequence) > self.max_len:\n",
    "                    # 자르기\n",
    "                    padded_seq = sequence[:self.max_len]\n",
    "                else:\n",
    "                    # 패딩 (앞쪽에 pad_token_id 추가)\n",
    "                    pad_length = self.max_len - len(sequence)\n",
    "                    padded_seq = [self.tokenizer.pad_token_id] * pad_length + sequence\n",
    "                \n",
    "                # 입력과 라벨 분리\n",
    "                X = padded_seq[:-1]\n",
    "                label = padded_seq[-1]\n",
    "                \n",
    "                all_X.append(X)\n",
    "                all_labels.append(label)\n",
    "        \n",
    "        return all_X, all_labels\n",
    "\n",
    "    def __len__(self): # dataset의 전체 길이 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx): # dataset 접근\n",
    "        X = self.X[idx]\n",
    "        label = self.label[idx]\n",
    "        return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63027b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def cleaning_text(text):\n",
    "    cleaned_text = re.sub( r\"[^a-zA-Z0-9.,@#!\\s']+\", \"\", text) # 특수문자 를 모두 지우는 작업을 수행합니다.\n",
    "    cleaned_text = cleaned_text.replace(u'\\xa0',u' ') # No-break space를 unicode 빈칸으로 변환\n",
    "    cleaned_text = cleaned_text.replace('\\u200a',' ') # unicode 빈칸을 빈칸으로 변환\n",
    "    return cleaned_text\n",
    "\n",
    "data = list(map(cleaning_text, data))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef61539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train set, validation set, test set으로 data set을 나눕니다. 8 : 1 : 1 의 비율로 나눕니다.\n",
    "train, test = train_test_split(data, test_size = .2, random_state = 42)\n",
    "val, test = train_test_split(test, test_size = .5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb41a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train 개수: \", len(train))\n",
    "print(\"Validation 개수: \", len(val))\n",
    "print(\"Test 개수: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c43d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train, tokenizer, max_len)\n",
    "valid_dataset = CustomDataset(val, tokenizer, max_len)\n",
    "test_dataset = CustomDataset(test, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ace40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ebe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordPredictionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims, hidden_dims, num_classes, dropout_ratio, set_super):\n",
    "        if set_super:\n",
    "            super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dims, padding_idx = 0) # padding index 설정 => gradient 계산에서 제외\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_classes = num_classes\n",
    "        for i in range(len(self.hidden_dims) - 1):\n",
    "            self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))\n",
    "\n",
    "            self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1]))\n",
    "\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "            self.layers.append(nn.Dropout(dropout_ratio))\n",
    "\n",
    "        self.classifier = nn.Linear(self.hidden_dims[-1], self.num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            x: [batch_size, sequence_len] # padding 제외\n",
    "        OUTPUT:\n",
    "            output : [batch_size, vocab_size]\n",
    "        '''\n",
    "        x = self.embedding(x) # [batch_size, sequence_len, embedding_dim]\n",
    "        x = torch.sum(x, dim=1) # [batch_size, embedding_dim] 각 문장에 대해 임베딩된 단어들을 합쳐서, 해당 문장에 대한 임베딩 벡터로 만들어줍니다.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        output = self.classifier(x) # [batch_size, num_classes]\n",
    "        output = self.softmax(output) # [batch_size, num_classes]\n",
    "        return output\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcef4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "  model.train()  # 모델을 학습 모드로 설정\n",
    "  train_loss = 0.0\n",
    "  train_accuracy = 0\n",
    "\n",
    "  tbar = tqdm(dataloader)\n",
    "  for texts, labels in tbar:\n",
    "      texts = texts.to(device)\n",
    "      labels = labels.to(device)\n",
    "      # 순전파\n",
    "      outputs = model(texts)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # 역전파 및 weights 업데이트\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # 손실과 정확도 계산\n",
    "      train_loss += loss.item()\n",
    "      # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "      # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "      tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "  # 에폭별 학습 결과 출력\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "  return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, valid_dataset, criterion, device, epoch, num_epochs):\n",
    "  model.eval()  # 모델을 평가 모드로 설정\n",
    "  valid_loss = 0.0\n",
    "  valid_accuracy = 0\n",
    "\n",
    "  with torch.no_grad(): # model의 업데이트 막기\n",
    "      tbar = tqdm(dataloader)\n",
    "      for images, labels in tbar:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          # 순전파\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          # 손실과 정확도 계산\n",
    "          valid_loss += loss.item()\n",
    "          # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "          # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "          tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "  valid_loss = valid_loss / len(dataloader)\n",
    "  valid_accuracy = valid_accuracy / len(valid_dataset)\n",
    "\n",
    "  return model, valid_loss, valid_accuracy\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, valid_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "          valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"../data/model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "embedding_dims = 512\n",
    "hidden_dims = [embedding_dims, embedding_dims*4, embedding_dims*2, embedding_dims]\n",
    "model = NextWordPredictionModel(vocab_size = vocab_size, embedding_dims = embedding_dims, hidden_dims = hidden_dims, num_classes = vocab_size, \\\n",
    "            dropout_ratio = 0.2, set_super = True).to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "model_name = 'next'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.NLLLoss(ignore_index=0) # padding 한 부분 제외\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "print('Valid max accuracy : ', valid_max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b520345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"../data/model_next.pt\")) # 모델 불러오기\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "total_labels = []\n",
    "total_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_dataloader):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(texts)\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_preds.extend(predicted.detach().cpu().tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "\n",
    "total_preds = np.array(total_preds)\n",
    "total_labels = np.array(total_labels)\n",
    "nwp_dnn_acc = accuracy_score(total_labels, total_preds) # 정확도 계산\n",
    "print(\"Next word prediction DNN model accuracy : \", nwp_dnn_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
